{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello dear! <3\n",
      "Switched to branch 'dev'\n",
      "Your branch is up to date with 'origin/dev'.\n",
      "Already up to date.\n",
      "Switched to branch 'feat/service_for_converting_docs'\n",
      "Collecting pypdfium2\n",
      "  Downloading pypdfium2-4.16.0-py3-none-manylinux_2_26_x86_64.whl (2.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdfium2\n",
      "Successfully installed pypdfium2-4.16.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install -U pypdfium2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello dear! <3\n",
      "Switched to branch 'dev'\n",
      "Your branch is up to date with 'origin/dev'.\n",
      "Already up to date.\n",
      "Switched to branch 'feat/service_for_converting_docs'\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.8/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.8/site-packages (from beautifulsoup4) (2.4.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install -U beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypdfium2 as pdfium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = pdfium.PdfDocument(\"spring.pdf\")\n",
    "version = pdf.get_version()  # get the PDF standard version\n",
    "n_pages = len(pdf)\n",
    "n_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_indices = [i for i in range(n_pages)]  # all pages\n",
    "renderer = pdf.render(\n",
    "    pdfium.PdfBitmap.to_pil,\n",
    "    page_indices = page_indices,\n",
    "    scale = 300/72,  # 300dpi resolution\n",
    ")\n",
    "for i, image in zip(page_indices, renderer):\n",
    "    image.save(\"out_%0*d.jpg\" % (2, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_doc_text = ''\n",
    "for page in range(n_pages):\n",
    "    page_index = pdf[page]\n",
    "    textpage = page_index.get_textpage()\n",
    "    text_all = textpage.get_text_range()\n",
    "    full_doc_text += text_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SPRING: GPT-4 Out-performs RL Algorithms by\\r\\nStudying Papers and Reasoning\\r\\nYue Wu14∗\\r\\n, Shrimai Prabhumoye2\\r\\n, So Yeon Min1\\r\\n, Yonatan Bisk1\\r\\n, Ruslan Salakhutdinov1\\r\\n,\\r\\nAmos Azaria3\\r\\n, Tom Mitchell1\\r\\n, Yuanzhi Li1,4\\r\\n1Carnegie Mellon University, 2NVIDIA, 3Ariel University, 4Microsoft Research\\r\\nAbstract\\r\\nOpen-world survival games pose significant challenges for AI algorithms due to\\r\\ntheir multi-tasking, deep exploration, and goal prioritization requirements. Despite\\r\\nreinforcement learning (RL) being popular for solving games, its high sample\\r\\ncomplexity limits its effectiveness in complex open-world games like Crafter or\\r\\nMinecraft. We propose a novel approach, SPRING, to read the game’s original\\r\\nacademic paper and use the knowledge learned to reason and play the game through\\r\\na large language model (LLM). Prompted with the LATEX source as game context\\r\\nand a description of the agent’s current observation, our SPRING framework em\\ufffeploys a directed acyclic graph (DAG) with game-related questions as nodes and\\r\\ndependencies as edges. We identify the optimal action to take in the environment\\r\\nby traversing the DAG and calculating LLM responses for each node in topological\\r\\norder, with the LLM’s answer to final nodedirectly translating to environment\\r\\nactions. In our experiments, we study the quality of in-context “reasoning” in\\ufffeduced by different forms of prompts under the setting of the Crafter open-world\\r\\nenvironment. Our experiments suggest that LLMs, when prompted with consistent\\r\\nchain-of-thought, have great potential in completing sophisticated high-level tra\\ufffejectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL\\r\\nbaselines, trained for 1M steps, without any training. Finally, we show the potential\\r\\nof games as a test bed for LLMs.\\r\\n1 Introduction\\r\\nOpen-world survival games like Minecraft Fan et al. (2022) and Crafter Hafner (2021) pose significant\\r\\nchallenges for AI algorithms due to a combination of factors: procedural generation requires strong\\r\\ngeneralization; diverse action space requires multi-task capabilities; technology tree requires long\\ufffeterm planning and deep exploration; diverse and conflicting objectives requires goal prioritization. In\\r\\nparticular, Crafter is designed for efficient simulation and fast iteration. Similar to Minecraft, Crafter\\r\\nfeatures key challenges such as multi-tasking, exploration with a deep and wide tech-tree, requiring\\r\\nthe agent to craft multiple tools and interact with multiple objects to survive in the game.\\r\\nReinforcement learning (RL) has been the go-to approach for game-based problems, with numerous\\r\\nsuccesses in games like Go Silver et al. (2017), robotics Fu et al. (2020); Hafner et al. (2023) and\\r\\nvarious video games Vinyals et al. (2019); Schrittwieser et al. (2020); Badia et al. (2020); Hafner\\r\\net al. (2023). While RL demonstrated impressive performance, it still suffers from certain limitations,\\r\\nsuch as high sample complexity and difficulty in incorporating prior knowledge. Such drawbacks\\r\\nmake it exceptionally challenging to apply RL to diverse and complex open-world benchmarks like\\r\\nCrafter Hafner (2021) or Minecraft Fan et al. (2022). Addressing the benefits and drawbacks of RL is\\r\\ntherefore crucial for achieving a sample-efficient solution.\\r\\n∗Work done during internship at Microsoft. For correspondence, contact ywu5@andrew.cmu.edu\\r\\nPreprint. Under review.\\r\\narXiv:2305.15486v2 [cs.AI] 29 May 2023Figure 1: Overview of SPRING. The context string, shown in the middle column, is obtained by\\r\\nparsing the LATEX source code of Hafner (2021). The LLM-based agent then takes input from a\\r\\nvisual game descriptor and the context string. The agent uses questions composed into a DAG for\\r\\nchain-of-thought reasoning, and the last node of the DAG is parsed into action.\\r\\nOn the other hand, large language models (LLMs) Brown et al. (2020); Smith et al. (2022); Chowdhery\\r\\net al. (2022) have shown remarkable success when prompted for various tasks, including embodied\\r\\nplanning and acting Ahn et al. (2022); Du et al. (2023); Wang et al. (2023); Shinn et al. (2023), QA\\r\\nor dialogue Ouyang et al. (2022); Bubeck et al. (2023), and general problem-solving Brown et al.\\r\\n(2020); Bubeck et al. (2023). Their unique planning Ahn et al. (2022), reasoning Shinn et al. (2023),\\r\\nand problem-solving Bubeck et al. (2023); Madaan et al. (2023) ability makes them a promising\\r\\ncandidate for incorporating prior knowledge and in-context reasoning for game-based problems,\\r\\nparticularly when it comes to addressing the aforementioned limitations of RL.\\r\\nHence, in this work, we study the possibility and reliability of LLMs for understanding and reasoning\\r\\nwith human knowledge, in the setting of games. We consider a two staged approach SPRING\\r\\n(Figure 1): (1) studying the paper: the first stage reads the LATEX of the paper of Hafner (2021) and\\r\\n(2) reasoning: the second stage involves reasoning about that knowledge through a QA framework to\\r\\ntake an environment action. Note that the Crafter environment was released after the data collection\\r\\ndate of GPT-3.5 and GPT 4 OpenAI (2023) models2\\r\\n, the environment is OOD to them. We first use\\r\\nLLM to extract prior knowledge from the LATEX source code of the original paper by Hafner (2021).\\r\\nWe then use a similar QA summarization framework as Wu et al. (2023) which produces QA dialogue\\r\\non game mechanics. SPRING handles significantly more diverse contextual information than Wu\\r\\net al. (2023), making use of all 17 action/interaction types and even information about desirable\\r\\nbehaviors documented in the paper.\\r\\nWe focus on reading the relevant academic paper in the first stage of SPRING, by first deciding\\r\\nthe paragraphs that are relevant for playing the game. Then we extract key information through\\r\\na series of questions such as “Write all information helpful for the game in a numbered list.\". In\\r\\nthe second stage,we promote and regulate in-context chain-of-thought reasoning in LLMs to solve\\r\\ncomplex games. The reasoning module is a directed acyclic graph (DAG), with questions as nodes\\r\\nand dependencies as edges. For example, the question “For each action, are the requirements met?\"\\r\\ndepends on the question “What are the top 5 actions?\", creating an edge from the latter to the former.\\r\\nFor each environment step, we traverse the DAG computing LLM answers for each node in the\\r\\ntopological order of the graph. The final node of the DAG is a question about the best action to take\\r\\nand the LLM answer for the question is directly translated to environment action.\\r\\nQualitatively, our experiments show that LLMs, when prompted with consistent chain-of-thought, can\\r\\nexecute sophisticated trajectories independently in.Quantitatively, SPRING’s zero-shot performance\\r\\nwith GPT-4 surpassing all state-of-the-art RL algorithmstrained for 1M steps (Table 2).\\r\\nOur contributions are as follows:\\r\\n• SPRING is the first to tackle a competitive RL benchmark by explicitly extracting multiple\\r\\ninteractions and tech-tree dependencies directly from an academic paper.\\r\\n2GPT-3.5/4 training data ends in September 2021 according to OpenAI API\\r\\n2• We are the first to show SOTA performance performance in a challenging open world game\\r\\nwith a zero-shot LLM-based (GPT-4) policy\\r\\n• We study the quality of in-context “reasoning” induced by different prompts and propose a\\r\\ncontrolled chain-of-thought prompting through a DAG of questions for decision making.\\r\\n2 Method\\r\\nThis section is structured as follows. We first describe how we generate the context from the LATEX\\r\\nsource code of Hafner (2021) in Section 2.1. Then we describe our SPRING framework and how we\\r\\ncompute the action in Section 2.2.\\r\\nProblem Setting Our goal is to show that LLMs can plan and act reasonably well in an environment\\r\\nwhere control tasks are less required. In the setting of Crafter, we define the states, s, as samples\\r\\nfrom state distribution S. We are interested in creating a goal-conditioned policy π which maps state\\r\\ns to action a ∈ A, π : S → A. Due to the use of LLM, we further break the policy down into two\\r\\nparts: a descriptor D which describes key aspects the visual observation in plain text (d = D(s)).\\r\\nAnd an LLM-based actor which takes state description d and outputs action a.\\r\\nIn addition, we define S\\r\\nj\\r\\npara to be the j\\r\\nth paragraph in the LATEX source of the environment paper\\r\\nHafner (2021), and MLLM to be the LLM which takes a context string and a question string as input\\r\\nand outputs an answer to the question.\\r\\n2.1 Studying the paper: Context from LATEX source\\r\\nFigure 2: Paper Studying Moudle. The 3-step approach for computing Cq from the LATEXsource\\r\\ncode of Hafner (2021). First, as shown in the left column, for each paragraph we compute LLM\\r\\nanswer for all relevancy questions in Qrel, and keep only the relevant paragraphs. Second, as shown in\\r\\nthe middle column, we compute paragraph-level LLM answer to q. Third, we summarize the answer\\r\\ninto Cq with a summary prompt; we concatenate Cq across q ∈ Qgame and obtain C.\\r\\nSimilar to Wu et al. (2023), we compose gameplay specific questions and then compute LLM answer\\r\\nto the questions for each subsection in the latex files. Since a considerable amount of the paper is\\r\\nirrelevant to the gameplay, we use a set of 2 questions Qrel={“Would this paragarph help me succeed in this game?\",\\r\\n“Does this paragraph contain information on the game mechanics, or game strategies?\"} to identify relevance, and a set of 4\\r\\nquestions Qgame={“Write all information helpful for the game in a numbered list.\", “In plain text. List all objects I need to interact/avoid to\\r\\nsurvive in the game. Use \"I would like to X object Y\" in each step. Replace Y by the actual object, X by the actual interaction.\", “Write all game\\r\\nobjectives numbered list. For each objective, list its requirements.\", “Write all actions as a numbered list. For each action, list its requirements.\"}\\r\\nto summarize gameplay and action space relevant information. We add the prompt “DO NOT answer in\\r\\nLaTeX.” to all of Qgame to prevent the LLM from outputting the list in LATEX format.\\r\\nFor a specific gameplay specific question q ∈ Qgame, our goal is to compute Cq, the answer to q\\r\\nconditioned on the paper. However, since the length of the paper exceeds input length constraints\\r\\nfor most LLMs, we have to break the paper down into paragraphs individual S\\r\\nj\\r\\npara. We provide an\\r\\nillustration of the process in Figure 2.\\r\\nFirst, we filter the paragraphs for relevance and keep only paragraphs identified as relevant by at least\\r\\none question from Qrel. We set P\\r\\nrel\\r\\nq\\r\\nto be the set of relevant paragraphs.\\r\\nP\\r\\nrel\\r\\nq =\\r\\n\\x08\\r\\nS\\r\\nj\\r\\npara|∃qr ∈ Qrel s.t. MLLM \\ufffe\\r\\nS\\r\\nj\\r\\npara, qr\\r\\n\\x01\\r\\n= “Yes”\\t\\r\\n(1)\\r\\n\\x00Second, we compute the set, Aq, of answers to q for each relevant paragraph from P\\r\\nrel\\r\\nq\\r\\n, from the\\r\\nLATEX source code.\\r\\nAq =\\r\\n\\x08MLLM (Spara, q) : Spara ∈ P\\r\\nrel\\r\\nq\\r\\n\\t\\r\\n(2)\\r\\nThird, to obtain the answer string Cq from the set Aq, we query an LLM with a summarization\\r\\nprompt qsummarize = “Remove duplicate items.”\\r\\nCq = MLLM (concat(Aq), qsummarize) (3)\\r\\nFinally, we concatenate (with the linebreak character) all question-context pairs to form the context\\r\\nstring C for SPRING.\\r\\nC = concat ({“Question: q Answer: Cq”|∀q ∈ Qgame}) (4)\\r\\n2.2 Reasoning: QA-DAG for SPRING\\r\\nNode Question\\r\\nq1\\r\\nList objects in the current observation. For each object, briefly answer what resource it provides\\r\\nand its requirement.\\r\\nq2 What was the last action taken by the player?\\r\\nq3 For each object in the list, are the requirements met for the interaction?\\r\\nq4 Did the last player action succeed? If not, why?\\r\\nq5 List top 3 sub-tasks the player should follow. Indicate their priority out of 5.\\r\\nq6 What are the requirements for the top sub-task? What should the player do first?\\r\\nq7\\r\\nList top 5 actions the player should take and the requirement for each action. Choose ONLY from\\r\\nthe list of all actions. Indicate their priority out of 5.\\r\\nq8 For each action in the list, are the requirements met?\\r\\nqa Choose the best executable action from above.\\r\\nTable 1: List of all 9 questions in Qact. The questions are designed to promote consistent chain-of\\ufffethought. Experimentally, we find the LLM robust to different phrasing of the questions.\\r\\nFigure 3: Reasoning. (a) The visual descriptor takes the last two gameplay screens as input, and\\r\\noutputs their descriptions in language (d\\r\\nt\\r\\n, dt−1\\r\\n). (b) SPRING traverses a DAG of questions from\\r\\nTable 1 in topological order. Answer to the final question qa is mapped to environment action using\\r\\nsub-string matching. (c) The LLM answer for each question (node) is conditioned on the previous 2\\r\\nsteps of observation, the context C, and answers to the immediate parents of the current node.\\r\\nFor LLMs to be able to understand the gameplay, we first follow Du et al. (2023); Wang et al. (2023)\\r\\nto define an visual descriptor Mdesc which converts state s ∈ S to textual description d (Figure 3 a).\\r\\nTo achieve consistent chain-of-thought reasoning Wei et al. (2021) throughout hundreds of steps\\r\\nwithin one round of gameplay, we compose a fixed set of questions Qact = {q1, . . . , qa} to query\\r\\n4the LLM at every step of the game, with question-question dependencies as D = {(qu, qv)|qu, qv ∈\\r\\nQact and answering qv requires the answer of qu}. Note that the above specification forms a directed\\r\\nacyclic graph (DAG) with nodes Qact edges D (Figure 3 b).\\r\\nFor any question (node) qv ∈ Qact, we compute the answer At\\r\\nqv\\r\\nfor time step t, conditioned on the\\r\\ngameplay context C, most recent 2 steps of game description d\\r\\nt−1\\r\\n, dt\\r\\n, and answers to its dependencies\\r\\n(Figure 3 c).\\r\\nA\\r\\nt\\r\\nqv = MLLM \\ufffe\\r\\nconcat \\ufffe\\r\\nC, dt−1\\r\\n, dt\\r\\n,\\r\\n\\x08\\r\\nA\\r\\nt\\r\\nqu\\r\\n|(qu, qv) ∈ D\\r\\n\\t\\x01 , qv\\r\\n\\x01\\r\\n(5)\\r\\nExperimentally, we find that prompting the LLM with only the direct parents of a question greatly\\r\\nreduces the context length, and helps LLM to focus on the most relevant contextual information.\\r\\nWe traverse the DAG using a modified topological sort algorithm to compute LLM answer for each\\r\\nquestion based on its topological order. Finally, we map the answer to the last question in the node qa\\r\\ndirectly to one of the 17 named actions in the environment with sub-string matching (a = At\\r\\na\\r\\n). We\\r\\ntake the default action “Do” on sub-string matching failure.3\\r\\n3 Experiments and Results\\r\\nWe present our experiments as follows. First, we explain our experimental setup and baselines for our\\r\\nexperiments. Then, we compare SPRING to popular RL methods on the Crafter benchmark. Finally,\\r\\nwe conduct experiments and analysis on different pieces of our architecture to study the influence of\\r\\neach part over the in-context “reasoning” capabilities of the LLM.\\r\\n3.1 Experimental Details\\r\\nThe Crafter environment Hafner (2021) is a procedurally generated open-world survival game for\\r\\nbenchmarking RL algorithms with 22 achievements in a tech tree of depth 7. The environment is a\\r\\ngrid-world features top-down observation and discrete action space of size 17. The observation also\\r\\nshows the current inventory state of the player, including its health points, food, water, rest levels,\\r\\nand inventory. The game is inspired by Minecraft and features a similar get-to-diamond challenge. In\\r\\ncomparison, Crafter captures many key research challenges of Minecraft in a more simple and fast\\r\\nenvironment, thus speeding up experiments and result collection.\\r\\nEnvironment Descriptor The gameplay screen (top left of Fig 3.) consists of a 9 × 9 grid\\r\\n({(i, j) | 1 ≤ i, j ≤ 9}). The top 7 rows consist of the local view of the world; each cell (i, j) is\\r\\nassociated with a pre-defined background (e.g., “grass”, “water”, “none”) and possibly with an object\\r\\n“asset” (e.g., “tree”, “health”, “player”). The bottom 2 rows represent agent status (e.g., “health”) and\\r\\nitem inventories, which include images of assets (e.g., “stone sword”), and the number of each assent\\r\\nin the inventory.\\r\\nOur environment descriptor accepts as input the gameplay screen and outputs a text description of\\r\\nthe screen. We first create combinations of background and object (appearance) assets. Then we\\r\\nadd number assets to recognize the quantity of inventory/ status. We match these combinations\\r\\nwith the gameplay screen, using cv2.filters with a matching threshold of 0.9. We disable the\\r\\ndetector during nights when observations are unreliable. Finally, for each (i, j), we filter the matched\\r\\ncombinations, and select the one with the highest matching score. From this information, we can\\r\\nmeasure the distance and direction of each object relative to the player; simultaneously, we can count\\r\\nthe agent status and inventory item.\\r\\nThe environment descriptor then obtains the set of objects in observation O =\\r\\n{(obj, dist, direction)}, the set of inventory items I = {(object, count)}, and the agent status\\r\\nH = {(attribute, value, max)}. Including only the closest object of each kind, we compose the\\r\\nobservation description d as: “You see : - <obj> <dist> steps to your <direction>. Your status:\\r\\n<attribute>: <value>/ <max>. Your inventory: - <object>: <count>”. We describe direction of objects\\r\\nusing “north”,“south”,“east”,“west”.\\r\\nEvaluation Metrics Agents in Crafter are evaluated primarily based on two metrics: reward and\\r\\nscore. The game assigns a sparse +1 reward each time the agent unlocks a new achievement in an\\r\\n3We will release code for our agent at github.com/anonymous\\r\\x00\\x00Method Score Reward Training Steps\\r\\nHuman Experts 50.5 ± 6.8% 14.3 ± 2.3 N/A\\r\\nSPRING + paper (Ours) 27.3 ± 1.2% 12.3 ± 0.7 0\\r\\nDreamerV3 Hafner et al. (2023) 14.5 ± 1.6% 11.7 ± 1.9 1M\\r\\nELLM Du et al. (2023) N/A 6.0 ± 0.4 5M\\r\\nEDE Jiang et al. (2022) 11.7 ± 1.0% N/A 1M\\r\\nDreamerV2 Hafner et al. (2020) 10.0 ± 1.2% 9.0 ± 1.7 1M\\r\\nPPO Schulman et al. (2017) 4.6 ± 0.3% 4.2 ± 1.2 1M\\r\\nRainbow Hessel et al. (2018) 4.3 ± 0.2% 5.0 ± 1.3 1M\\r\\nPlan2Explore Sekar et al. (2020) 2.1 ± 0.1% 2.1 ± 1.5 1M\\r\\nRND Burda et al. (2018) 2.0 ± 0.1% 0.7 ± 1.3 1M\\r\\nRandom 1.6 ± 0.0% 2.1 ± 1.3 0\\r\\nTable 2: Table comparing SPRING and popular RL algorithms in terms of game score, reward, and\\r\\ntraining steps. Results for SPRING is summarized over 5 independent trials. SPRING out-performs\\r\\nthe previous SOTA in terms of all metrics. In addition, since SPRING gathers knowledge from\\r\\nreading the paper, it requires no training.\\r\\nepisode, and assigns reward of −0.1/0.1 when the agent loses/gains one health point. The score\\r\\nmetric Hafner (2021) is computed by aggregating the success rates for each achievement:\\r\\nS = exp \\r\\n1\\r\\nN\\r\\nX\\r\\nN\\r\\ni=1\\r\\nln (1 + si)\\r\\n!\\r\\n− 1,\\r\\nwhere si\\r\\nis the agent’s success rate on achievement i and N = 22 is the number of achievements.\\r\\nNote that RL agents only train on the reward, and SPRING does not require any training.\\r\\nRL Baselines We include results from popular actor-critic methods like PPO Schulman et al.\\r\\n(2017); DQN variants like Rainbow Hessel et al. (2018); intrinsically motivated methods like\\r\\nRND Burda et al. (2018), Plan2Explore Sekar et al. (2020), EDE Jiang et al. (2022); LLM assisted\\r\\nsolutions like ELLM Du et al. (2023); model-based methods like DreamerV2 Hafner et al. (2020);\\r\\nDreamerV3 Hafner et al. (2023), which currently holds the state-of-the-art.\\r\\nLLMs. For LLM access, we use GPT-3.5-turbo OpenAI (OpenAI) and GPT-4 OpenAI (2023) from\\r\\nOpenAI’s API.\\r\\n3.2 Overall Results\\r\\nWe compare the performance of RL baselines to SPRING with GPT-4 conditioned on the environment\\r\\npaper Hafner (2021) in Table 2.\\r\\nSPRING out-performs the previous SOTA, including previous attempts at using LLMs for Crafter\\r\\nby large margins, achieving an 88% relative improvement on game score and a 5% improvement in\\r\\nreward on the best performing RL method Hafner et al. (2023). Since the model obtains knowledge\\r\\nfrom reading the paper, SPRING requires 0 training steps, while RL methods generally require\\r\\nmillions of training steps.\\r\\nWe include a plot of unlock rate by task, comparing our method to popular RL baselines in Figure 4.\\r\\nSPRING assisted by prior knowledge out-performs RL methods by more than 10x on achievements\\r\\nlike “Make Stone Pickaxe”, “Make Stone Sword”, and “Collect Iron”, which are up to depth 5 down\\r\\nin the tech tree and significantly harder to reach through random exploration. For achievements\\r\\n“Eat Cow” and “Collect Drink”, SPRING achieves perfect performance, whereas model-based RL\\r\\nframework like Dreamer-V3 has more than 5x lower unlock rate for “eat cow” since cows are moving\\r\\nand harder to reach through random exploration. Finally, we note that SPRING did not take the action\\r\\n“Place Stone”, which can be reached easily by random exploration, since placing a stone was not\\r\\ndiscussed as beneficial for the agent in the paper Hafner (2021).\\r\\n3.3 Component Analysis\\r\\nWe study how the different aspects of the framework contribute to the behavior of the agent through a\\r\\nseries of ablations as shown in Table 3.\\r\\n6Collect Coal\\r\\nCollect Diamond\\r\\nCollect Drink\\r\\nCollect Iron\\r\\nCollect Sapling\\r\\nCollect Stone\\r\\nCollect Wood\\r\\nDefeat Skeleton\\r\\nDefeat Zombie\\r\\nEat Cow\\r\\nEat Plant\\r\\nMake Iron Pickaxe\\r\\nMake Iron Sword\\r\\nMake Stone Pickaxe\\r\\nMake Stone Sword\\r\\nMake Wood Pickaxe\\r\\nMake Wood Sword\\r\\nPlace Furnace\\r\\nPlace Plant\\r\\nPlace Stone\\r\\nPlace Table\\r\\nWake Up\\r\\n0.01\\r\\n0.1\\r\\n1\\r\\n10\\r\\n100\\r\\nSuccess Rate (%)\\r\\nOurs DreamerV3 Rainbow\\r\\nFigure 4: Ability spectrum showing the unlocking percentages for all 22 achievements. Rainbow\\r\\nmanages to drink water and forage for food. DreamerV3 collects coal, iron, stone, and forges more\\r\\nadvanced tools and weapons. Since SPRING starts off with knowledge about the game, it achieves\\r\\nmore than 10x higher unlock rate on previously hard-to-reach tasks like “Eat Plant”, “Make Stone\\r\\nPickaxe”, “Make Stone Sword”, and “Collect Iron”.\\r\\nMethod Achievement Depth Reward Questions per Step\\r\\nSPRING + Full Paper 6 12.3 ± 0.7 9\\r\\nSPRING + Paper w/ modified C 4 9.4 ± 1.8 9\\r\\nSPRING + Action Description 4 8.2 ± 0.2 9\\r\\nSPRING + w/o C 1 0.5 ± 0.2 9\\r\\nSPRING + Full Paper 6 12.3 ± 0.7 9\\r\\nStep-by-step prompt + Full Paper 5 7.3 ± 4.4 2\\r\\nQA w/o DAG + Full Paper 4 4.3 ± 3.9 9\\r\\nw/o QA + Full Paper 2 2.4 ± 1.3 1\\r\\nSPRING + Full Paper 6 12.3 ± 0.7 9\\r\\nSPRING + Full Paper w/ GPT-3.5 2 3.3 ± 2.9 9\\r\\nTable 3: Analysis on how different parts of SPRING contribute to its performance, comparing the\\r\\nmax achievement depth in the tech tree, the reward, and the number of human-written questions in\\r\\nthe prompt. Results are summarized over 5 independent trials. The first 4 rows study the necessity\\r\\nof prior knowledge from the context string C. The middle 4 rows study different chain-of-thought\\r\\nprompting techniques. The last 2 rows study the role of LLMs. All three aspects are important for\\r\\nSPRING to achieve best reported performance.\\r\\nStudying the LATEX Paper In the first 4 rows of Table 3, we investigate the contribution of\\r\\ngameplay context from the LATEX paper toward performance of the agent. We report the performance\\r\\nof SPRING with no contextual information (w/o C) (row 4); SPRING conditioned on only the action\\r\\ndescriptions and dependencies from Hafner (2021) Table F.1 (only question 4 from Qgame) (row 3);\\r\\nSPRING conditioned on the context manually modified to exclude the “crafting table” dependency\\r\\nfor wooden_pickaxe by removing two corresponding lines from the context C (row 2); SPRING\\r\\nconditioned on the full context from the paper (row 1).\\r\\nAs expected, since Crafter environment is OOD for GPT, the agent achives performance similar\\r\\nto random agent without any game context. When provided with only action descriptions and\\r\\naction dependencies, using only question 4 from Qgame in section 2.1, SPRING achieves strong 67%\\r\\nperformance comparable to DreamerV2 Silver et al. (2017).\\r\\nFor the next piece of the experiment, we manually remove “near crafting table” dependency for\\r\\nwooden_pickaxe from it’s context, which is required for 11 later achievements. SPRING with GPT-4\\r\\nincurs a 24% performance drop. Interestingly, we find that the LLM has some ability to recover from\\r\\nthe inaccurate context information. We observe that after failing to craft the wooden_pickaxe without\\r\\na table, the agent instead tries to craft a wooden_sword first to maintain survival. Eventually, the agent\\r\\nwas able to identify the missing requirement through guessing and trying after some unsuccessful\\r\\ntrials, and craft the wooden_pickaxe. However, the confusion delayed the agent’s progress and\\r\\ntherefore causes the performance gap with the agent conditioned on the full context (row 5).\\r\\n7Method Achievement Depth Reward Questions per Step\\r\\nStep-by-step prompt + GPT-4 5 7.3 ± 4.4 2\\r\\nStep-by-step prompt + text-davinci-003 4 4.5 ± 2.1 2\\r\\nStep-by-step prompt + Bard 0 −0.9 ± 0 2\\r\\nStep-by-step prompt + Claude 1 0.1 ± 0.1 2\\r\\nStep-by-step prompt + Alpaca-30b 1 0.1 ± 0.1 2\\r\\nRandom 1 2.1 ± 1.3 0\\r\\nTable 4: Comparison of different LLMs under the same setting using the context C generated with\\r\\ntext-davinci-003 following the same step-by-step prompt as Section 3.3 and Table 3.\\r\\nReasoning In the middle 4 rows of Table 3, we investigate the contribution of different prompting\\r\\nmethods toward performance of the model. Conditioned on the full context from the LATEX paper, we\\r\\nreport the performance of GPT-4 directly prompted to output the action using the last question qa only\\r\\n(row 8); GPT-4 prompted with all questions from Qact but in a list without the DAG dependencies D\\r\\n(row 7); GPT-4 prompted “Let’s think step-by-step” Kojima et al. (2022) about the next action, and\\r\\nprompted to choose a permissible action qa with let’s think step-by-step followed by qa again (row\\r\\n6); GPT-4 with SPRING (row 5).\\r\\nRelative to our method, we observe that directly prompting the LLM for the action leads to a 80%\\r\\nperformance drop, and therefore does not result in a meaningful agent. The popular chain-of-thought\\r\\nreasoning prompt “Let’s think step-by-step” Kojima et al. (2022) achieves reasonable reward with\\r\\na 40% drop, but with a high 60.27% standard deviation. Qualitatively, we observe that the LLM\\r\\nproduces inconsistent outputs across time steps, due to the fact that the model’s chain-of-thought\\r\\nis not directed or controlled through the prompt. Therefore, LLMs prompted with “Let’s think\\r\\nstep-by-step” alone cannot reliably follow a good policy. Controlling the chain-of-thought with 9\\r\\nquestions from Qact (section 2.2) successfully controls the consistency of LLM outputs across time\\r\\nqualitatively. However, we observe that the LLM often ignores earlier questions at later stages of QA\\r\\nwhen all previous questions are presented in a list, leading to random disagreements in answers. For\\r\\nexample, the LLM may correctly identify that it needs “wooden pickaxe” to mine the stone ahead\\r\\nin the first few questions, but forgets about the requirement later when it’s prompted for actions.\\r\\nQuantitatively, the model performs 65% worse with 90% variance without the DAG. The introduction\\r\\nof DAG eliminates this problem by reducing the QA context length to only a question’s immediate\\r\\nparents.\\r\\nOverall, SPRING achieves the best performance and a small 6% performance standard deviation, due\\r\\nto more consistent reasoning over time steps with better focus and fewer distractions.\\r\\nLLM In the last two rows of Table 3, we show that the same architecture does not work well with\\r\\nGPT-3.5-turbo. We believe the observed 73% performance gap mainly comes from GPT-3.5-turbo’s\\r\\nworse performance at following fine-grained instructions in each of the questions, which are required\\r\\nfor chain-of-thought reasoning with SPRING.\\r\\n3.4 Potential for Benchmarking LLMs\\r\\nIn Table 4, we compare popular publicly available LLMs including GPT-4 OpenAI (2023), GPT-3.5\\r\\n(text-davinci-003) OpenAI (OpenAI), Bard Manyika (Manyika), Claude Anthropic (Anthropic),\\r\\nAlpaca-30b Taori et al. (2023) under the same setting on Crafter, following the same step-by-step\\r\\nprompt as Section 3.3 and Table 3. We observe a clear separation in performance under our setting.\\r\\n4 Related Work\\r\\nPolicy Informed by Natural Language Instructions In the instruction following setting, step-by\\ufffestep instructions have been used to generate auxiliary rewards, when environment rewards are sparse.\\r\\nGoyal et al. (2019); Wang et al. (2019) use auxiliary reward-learning modules trained offline to\\r\\npredict whether trajectory segments correspond to natural language annotations of expert trajectories.\\r\\nThere has been many attempts to go beyond instruction following to learning from unstructured\\r\\nnatural language Branavan et al. (2012); Goldwasser and Roth (2014); Zhong et al. (2021); Wang\\r\\nand Narasimhan (2021). Zhong et al. (2021); Wang and Narasimhan (2021) make use of special\\r\\narchitectures to learn reasoning on grid worlds with template-generated instructions. However, the\\r\\n8model requires 200 million training samples from templates identical to the test environments. Such\\r\\na training requirement limiting the generalization of the model and causes performance loss even on\\r\\nslightly bigger grid worlds with identical mechanics.\\r\\nWu et al. (2023) proposes a summary (Read) and reasoning (Reward) through a QA prompting\\r\\nframework with an open-source QA LLM Tafjord and Clark (2021). The framework demonstrates\\r\\nthe possibility of an using real-world human-written manuals to improve RL performance on popular\\r\\ngames, despite limiting the interaction types to only “hit”. Our framework handles all 17 kinds of\\r\\ninteractions available in the game. Moreover, our framework makes use of information on tech-tree\\r\\ndependencies, and suggestions on desired policies extracted from the academic paper.\\r\\nLLMs for Planning LLMs have shown promising results at high-level planning in indoor embodied\\r\\nmanipulation environments. Huang et al. (2022); Ahn et al. (2022) primarily explores generating\\r\\nplans for embodied tasks, with limited actions space and trajectory length. Song et al. (2022); Wu\\r\\net al. (2022) enhances Ahn et al. (2022) with greater action diversity and real-time re-planning.\\r\\nHowever, a lot of the high-level plans lack executability and has to be post-processed to meet specific\\r\\ntask requirements, thus limiting the generalization to complex open world tasks. In addition, all\\r\\nprior works along this line operates on few-shot human/expert generated demonstrations containing\\r\\nup to 17 trajectories to provide context for LLMs, which requires more manual labor, and may\\r\\nlimit the generalization to unseen scenarios. In comparison, our SPRING framework requires no\\r\\ndemonstration.\\r\\nLLMs for Open World Games Compared to popular indoor manipulation tasks, planning in open\\ufffeworld game environments poses the following additional challenges. 1) Long horizon. Due to the\\r\\nnature how in-game achievement/technology progresses, a successful gameplay can easily go beyond\\r\\n200 steps Hafner (2021). 2) Parallel objectives. Open-world environments contain objectives that\\r\\ncan be pursued in parallel and often require prioritization Wang et al. (2023). Therefore, open world\\r\\ngames are significantly more challenging than current indoor embodied manipulation environments.\\r\\nDu et al. (2023) applies LLMs as high-level planners to assist RL exploration in Crafter. Wang\\r\\net al. (2023); Yuan et al. (2023) use LLMs as high-level planner and goal selector to control a low\\r\\nlevel-policy in Minecraft. Tsai et al. (2023) studies the capabilities of ChatGPT on text games.\\r\\nNotably, all prior works require expert or human generated example trajectories as context for the\\r\\nLLMs. Since the example trajectories do not cover all scenarios, all prior works may encounter\\r\\nunseen situation during evaluation, leading to an overall performance inferior to state-of-the-art RL\\r\\nalgorithms Hessel et al. (2018); Guss et al. (2021); Hafner et al. (2023), trained without the use\\r\\nof LLMs. To our knowledge, we are the first to show an LLM (GPT-4) achieving performance\\r\\nsurpassing the state-of-the-art RL algorithms in a challenging open world game.\\r\\n5 Limitations and Future Work\\r\\nA primary limitation in using an LLM to support interaction with the environment is the need for\\r\\nobject recognition and grounding. However, these limitations do not exist in environments that offer\\r\\naccurate object information, such as contemporary games Fan et al. (2022) and virtual reality worlds\\r\\nKolve et al. (2017). While pre-trained visual backbones He et al. (2017) perform poorly on games,\\r\\nthey have shown reasonable performance for environments closer to the real-world Shridhar et al.\\r\\n(2020). In addition, with recent progress on visual-language models Bubeck et al. (2023); Driess\\r\\net al. (2023); Liu et al. (2023); Zou et al. (2023), we believe there will be reliable and generalizable\\r\\nsolutions to visual-language understanding in the foreseeable future. Future works could focus on\\r\\naddress the requirement for a separate visual descriptor with large visual-language models.\\r\\n6 Conclusions\\r\\nIn this work, we explore solving the Crafter Hafner (2021) RL benchmark using the latest LLMs by\\r\\nreading the LATEX source code of an academic paper about the benchmark. We study the quality of\\r\\nin-context “reasoning” and “planning” induced by different forms of prompts under the setting of\\r\\nthe Crafter open-world environment. To enforce consistent planning and execution over hundreds of\\r\\nenvironment steps, we introduce SPRING, an innovative prompting framework for LLMs designed\\r\\n9to enable in-context chain-of-thought planning and reasoning. Quantitatively, SPRING with GPT-4\\r\\noutperforms all state-of-the-art RL baselines, trained for 1M steps, without any training.\\r\\nOur work demonstrates the reliability of LLMs for understanding and reasoning with human knowl\\ufffeedge. We hope that our work points to a new way of integrating human prior knowledge into RL\\r\\ntraining through intrinsic rewards Wu et al. (2023), hierarchical RL Shu et al. (2017), or sub-goal\\r\\nplanning Wang et al. (2023); Wu et al. (2023).\\r\\nBroader Impacts\\r\\nOur research on LLM holds potential for both positive and negative impacts. The benefits include\\r\\nbetter understanding of the powers of LLM and enhanced integration of prior knowledge, which could\\r\\nlead to advancement in various AI topics. However, the risks may involve reliance on computationally\\r\\ndemanding models, game cheating or exploitation, and reliance of prior knowledge.\\r\\nReferences\\r\\nAhn, M., A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan,\\r\\nK. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey,\\r\\nS. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine, Y. Lu, L. Luu,\\r\\nC. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers,\\r\\nC. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng (2022). Do\\r\\nas i can, not as i say: Grounding language in robotic affordances.\\r\\nAhn, M., A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan,\\r\\nK. Hausman, A. Herzog, et al. (2022). Do as i can, not as i say: Grounding language in robotic\\r\\naffordances. arXiv preprint arXiv:2204.01691.\\r\\nAnthropic. Introducing claude. https://www.anthropic.com/index/introducing-claude.\\r\\nAccessed: May 27, 2023.\\r\\nBadia, A. P., B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi, Z. D. Guo, and C. Blundell\\r\\n(2020). Agent57: Outperforming the atari human benchmark. In International Conference on\\r\\nMachine Learning, pp. 507–517. PMLR.\\r\\nBranavan, S., D. Silver, and R. Barzilay (2012). Learning to win by reading manuals in a monte-carlo\\r\\nframework. Journal of Artificial Intelligence Research 43, 661–704.\\r\\nBrown, T. B., B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\\r\\nG. Sastry, A. Askell, et al. (2020). Language models are few-shot learners. arXiv preprint\\r\\narXiv:2005.14165.\\r\\nBubeck, S., V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li,\\r\\nS. Lundberg, et al. (2023). Sparks of artificial general intelligence: Early experiments with gpt-4.\\r\\narXiv preprint arXiv:2303.12712.\\r\\nBurda, Y., H. Edwards, A. Storkey, and O. Klimov (2018). Exploration by random network distillation.\\r\\narXiv preprint arXiv:1810.12894.\\r\\nChowdhery, A., S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung,\\r\\nC. Sutton, S. Gehrmann, et al. (2022). Palm: Scaling language modeling with pathways. arXiv\\r\\npreprint arXiv:2204.02311.\\r\\nDriess, D., F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,\\r\\nQ. Vuong, T. Yu, et al. (2023). Palm-e: An embodied multimodal language model. arXiv preprint\\r\\narXiv:2303.03378.\\r\\nDu, Y., O. Watkins, Z. Wang, C. Colas, T. Darrell, P. Abbeel, A. Gupta, and J. Andreas (2023).\\r\\nGuiding pretraining in reinforcement learning with large language models. arXiv preprint\\r\\narXiv:2302.06692.\\r\\n10Fan, L., G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu, and\\r\\nA. Anandkumar (2022). Minedojo: Building open-ended embodied agents with internet-scale\\r\\nknowledge. arXiv preprint arXiv:2206.08853.\\r\\nFu, J., A. Kumar, O. Nachum, G. Tucker, and S. Levine (2020). D4rl: Datasets for deep data-driven\\r\\nreinforcement learning. arXiv preprint arXiv:2004.07219.\\r\\nGoldwasser, D. and D. Roth (2014). Learning from natural instructions. Machine learning 94,\\r\\n205–232.\\r\\nGoyal, P., S. Niekum, and R. J. Mooney (2019). Using natural language for reward shaping in\\r\\nreinforcement learning. arXiv preprint arXiv:1903.02020.\\r\\nGuss, W. H., M. Y. Castro, S. Devlin, B. Houghton, N. S. Kuno, C. Loomis, S. Milani, S. Mohanty,\\r\\nK. Nakata, R. Salakhutdinov, et al. (2021). The minerl 2020 competition on sample efficient\\r\\nreinforcement learning using human priors. arXiv preprint arXiv:2101.11071.\\r\\nHafner, D. (2021). Benchmarking the spectrum of agent capabilities. arXiv preprint\\r\\narXiv:2109.06780.\\r\\nHafner, D., T. Lillicrap, M. Norouzi, and J. Ba (2020). Mastering atari with discrete world models.\\r\\narXiv preprint arXiv:2010.02193.\\r\\nHafner, D., J. Pasukonis, J. Ba, and T. Lillicrap (2023). Mastering diverse domains through world\\r\\nmodels. arXiv preprint arXiv:2301.04104.\\r\\nHe, K., G. Gkioxari, P. Dollár, and R. Girshick (2017). Mask r-cnn. In Proceedings of the IEEE\\r\\ninternational conference on computer vision, pp. 2961–2969.\\r\\nHessel, M., J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot,\\r\\nM. Azar, and D. Silver (2018). Rainbow: Combining improvements in deep reinforcement learning.\\r\\nIn Thirty-second AAAI conference on artificial intelligence.\\r\\nHuang, W., P. Abbeel, D. Pathak, and I. Mordatch (2022). Language models as zero-shot planners:\\r\\nExtracting actionable knowledge for embodied agents.\\r\\nJiang, Y., J. Z. Kolter, and R. Raileanu (2022). Uncertainty-driven exploration for generalization in\\r\\nreinforcement learning. In Deep Reinforcement Learning Workshop NeurIPS 2022.\\r\\nKojima, T., S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa (2022). Large language models are\\r\\nzero-shot reasoners. arXiv preprint arXiv:2205.11916.\\r\\nKolve, E., R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, D. Gordon, Y. Zhu, A. Gupta,\\r\\nand A. Farhadi (2017). AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv.\\r\\nLiu, S., Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su, J. Zhu, et al. (2023).\\r\\nGrounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv\\r\\npreprint arXiv:2303.05499.\\r\\nMadaan, A., N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye,\\r\\nY. Yang, et al. (2023). Self-refine: Iterative refinement with self-feedback. arXiv preprint\\r\\narXiv:2303.17651.\\r\\nManyika, J. An overview of bard: an early experiment with generative ai. https://ai.google/\\r\\nstatic/documents/google-about-bard.pdf. Accessed: May 27, 2023.\\r\\nOpenAI. Gpt-3.5. https://platform.openai.com/docs/models/gpt-3-5. Accessed: May\\r\\n27, 2023.\\r\\nOpenAI (2023). Gpt-4 technical report.\\r\\nOuyang, L., J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\r\\nK. Slama, A. Ray, et al. (2022). Training language models to follow instructions with human\\r\\nfeedback. Advances in Neural Information Processing Systems 35, 27730–27744.\\r\\n11Schrittwieser, J., I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart,\\r\\nD. Hassabis, T. Graepel, et al. (2020). Mastering atari, go, chess and shogi by planning with a\\r\\nlearned model. Nature 588(7839), 604–609.\\r\\nSchulman, J., F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017). Proximal policy optimization\\r\\nalgorithms. arXiv preprint arXiv:1707.06347.\\r\\nSekar, R., O. Rybkin, K. Daniilidis, P. Abbeel, D. Hafner, and D. Pathak (2020). Planning to\\r\\nexplore via self-supervised world models. In International Conference on Machine Learning, pp.\\r\\n8583–8592. PMLR.\\r\\nShinn, N., B. Labash, and A. Gopinath (2023). Reflexion: an autonomous agent with dynamic\\r\\nmemory and self-reflection. arXiv preprint arXiv:2303.11366.\\r\\nShridhar, M., X. Yuan, M.-A. Côté, Y. Bisk, A. Trischler, and M. Hausknecht (2020). Alfworld: Align\\ufffeing text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768.\\r\\nShu, T., C. Xiong, and R. Socher (2017). Hierarchical and interpretable skill acquisition in multi-task\\r\\nreinforcement learning. arXiv preprint arXiv:1712.07294.\\r\\nSilver, D., J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,\\r\\nM. Lai, A. Bolton, et al. (2017). Mastering the game of go without human knowledge. na\\ufffeture 550(7676), 354–359.\\r\\nSmith, S., M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye,\\r\\nG. Zerveas, V. Korthikanti, E. Zheng, R. Child, R. Y. Aminabadi, J. Bernauer, X. Song, M. Shoeybi,\\r\\nY. He, M. Houston, S. Tiwary, and B. Catanzaro (2022). Using deepspeed and megatron to train\\r\\nmegatron-turing NLG 530b, A large-scale generative language model. CoRR abs/2201.11990.\\r\\nSong, C. H., J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su (2022). Llm-planner:\\r\\nFew-shot grounded planning for embodied agents with large language models. arXiv preprint\\r\\narXiv:2212.04088.\\r\\nTafjord, O. and P. Clark (2021). General-purpose question-answering with macaw. arXiv preprint\\r\\narXiv:2109.02593.\\r\\nTaori, R., I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto (2023).\\r\\nStanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/\\r\\nstanford_alpaca. Accessed: May 27, 2023.\\r\\nTsai, C. F., X. Zhou, S. S. Liu, J. Li, M. Yu, and H. Mei (2023). Can large language models play text\\r\\ngames well? current state-of-the-art and open questions. arXiv preprint arXiv:2304.02868.\\r\\nVinyals, O., I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi,\\r\\nR. Powell, T. Ewalds, P. Georgiev, et al. (2019). Grandmaster level in starcraft ii using multi-agent\\r\\nreinforcement learning. Nature 575(7782), 350–354.\\r\\nWang, H. and K. Narasimhan (2021). Grounding language to entities and dynamics for generalization\\r\\nin reinforcement learning. arXiv preprint arXiv:2101.07393.\\r\\nWang, X., Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y.-F. Wang, W. Y. Wang, and L. Zhang\\r\\n(2019). Reinforced cross-modal matching and self-supervised imitation learning for vision\\ufffelanguage navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\nRecognition, pp. 6629–6638.\\r\\nWang, Z., S. Cai, A. Liu, X. Ma, and Y. Liang (2023). Describe, explain, plan and select: Interactive\\r\\nplanning with large language models enables open-world multi-task agents. arXiv preprint\\r\\narXiv:2302.01560.\\r\\nWei, J., M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le (2021).\\r\\nFinetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\\r\\nWu, Y., Y. Fan, P. P. Liang, A. Azaria, Y. Li, and T. M. Mitchell (2023). Read and reap the rewards:\\r\\nLearning to play atari with the help of instruction manuals. arXiv preprint arXiv:2302.04449.\\r\\n12Wu, Y., S. Y. Min, Y. Bisk, R. Salakhutdinov, A. Azaria, Y. Li, T. Mitchell, and S. Prabhumoye\\r\\n(2023). Plan, eliminate, and track–language models are good teachers for embodied agents. arXiv\\r\\npreprint arXiv:2305.02412.\\r\\nWu, Y., S. Y. Min, Y. Bisk, R. Salakhutdinov, and S. Prabhumoye (2022). Tackling alfworld with\\r\\naction attention and common sense from language models.\\r\\nYuan, H., C. Zhang, H. Wang, F. Xie, P. Cai, H. Dong, and Z. Lu (2023). Plan4mc: Skill reinforcement\\r\\nlearning and planning for open-world minecraft tasks. arXiv preprint arXiv:2303.16563.\\r\\nZhong, V., A. W. Hanjie, S. I. Wang, K. Narasimhan, and L. Zettlemoyer (2021). Silg:\\r\\nThe multi-environment symbolic interactive language grounding benchmark. arXiv preprint\\r\\narXiv:2110.10661.\\r\\nZou, X., J. Yang, H. Zhang, F. Li, L. Li, J. Gao, and Y. J. Lee (2023). Segment everything everywhere\\r\\nall at once. arXiv preprint arXiv:2304.06718.\\r\\n13'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndeeppavlov/dream: DeepPavlov Dream is a free, open-source Multiskill AI Assistant Platform built using DeepPavlov Conversational AI Stack. It is built on top of DeepPavlov Agent running as container in Docker. It runs on x86_64 machines, and prefers having NVIDIA GPUs on the machine.\\n\\nSkip to content\\n\\n        Global navigation\\n\\n          Home\\n\\n          Issues\\n\\n          Pull requests\\n\\n          Discussions\\n\\n          Explore\\n\\n          Marketplace\\n\\n          Give new navigation feedback\\n\\nBeta\\n\\n © 2023 GitHub, Inc.\\n\\nAbout\\nBlog\\nTerms\\nPrivacy\\nSecurity\\nStatus\\n\\ndeeppavlov\\n\\xa0/\\n\\ndream\\n\\n        Navigate back to\\n\\n          deeppavlov\\n\\n          dream\\n\\n          deeppavlov\\n\\n/\\n\\n          dream\\n\\n                  Type / to search\\n\\nCommand palette\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n        Search\\n\\nClear\\n\\n              Search syntax tips\\n\\nGive feedback\\n\\n        Provide feedback\\n\\nWe read every piece of feedback, and take your input very seriously.\\n\\nInclude my email address so I can be contacted\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n        Saved searches\\n\\nUse saved searches to filter your results more quickly\\n\\nName\\n\\nQuery\\n\\n            To see all available qualifiers, see our documentation.\\n\\n     Cancel\\n\\n    Create saved search\\n\\n Create new...\\n\\n              New repository\\n\\n                Import repository\\n\\n                New codespace\\n\\n              New gist\\n\\n                New organization\\n\\n Issues\\n\\n Pull requests\\n\\nNotifications\\n\\n        Account menu\\n\\n            smilni\\n\\n            Nika Smilga\\n\\n    Loading...\\n\\n          Your profile\\n\\n          Your repositories\\n\\n          Your projects\\n\\n          Your codespaces\\n\\n    Loading...\\n\\n          Your stars\\n\\n          Your sponsors\\n\\n          Your gists\\n\\n    Loading...\\n\\n    Loading...\\n\\n    Loading...\\n\\n          Settings\\n\\n          GitHub Docs\\n\\n          GitHub Support\\n\\n          Sign out\\n\\nCode\\n\\nIssues\\n10\\n\\nPull requests\\n19\\n\\nDiscussions\\n\\nActions\\n\\nProjects\\n0\\n\\nWiki\\n\\nSecurity\\n\\nInsights\\n\\nMore\\n\\n                  Code\\n\\n                  Issues\\n\\n                  Pull requests\\n\\n                  Discussions\\n\\n                  Actions\\n\\n                  Projects\\n\\n                  Wiki\\n\\n                  Security\\n\\n                  Insights\\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n\\n/\\xa0\\xa0...\\xa0\\xa0/\\xa0\\xa0\\ndeeppavlov\\xa0\\xa0/\\xa0\\xa0\\ndream\\xa0\\xa0/\\xa0\\xa0\\n\\n Clear Command Palette\\n\\nTip:\\n                  Type # to search pull requests\\n\\n                Type ? for help and tips\\n\\nTip:\\n                  Type # to search issues\\n\\n                Type ? for help and tips\\n\\nTip:\\n                  Type # to search discussions\\n\\n                Type ? for help and tips\\n\\nTip:\\n                  Type ! to search projects\\n\\n                Type ? for help and tips\\n\\nTip:\\n                  Type @ to search teams\\n\\n                Type ? for help and tips\\n\\nTip:\\n                  Type @ to search people and organizations\\n\\n                Type ? for help and tips\\n\\nTip:\\n                  Type > to activate command mode\\n\\n                Type ? for help and tips\\n\\nTip:\\n                  Go to your accessibility settings to change your keyboard shortcuts\\n\\n                Type ? for help and tips\\n\\nTip:\\n                  Type author:@me to search your content\\n\\n                Type ? for help and tips\\n\\nTip:\\n                  Type is:pr to filter to pull requests\\n\\n                Type ? for help and tips\\n\\nTip:\\n                  Type is:issue to filter to issues\\n\\n                Type ? for help and tips\\n\\nTip:\\n                  Type is:project to filter to projects\\n\\n                Type ? for help and tips\\n\\nTip:\\n                  Type is:open to filter to open content\\n\\n                Type ? for help and tips\\n\\n            We’ve encountered an error and some results aren\\'t available at this time. Type a new search or try again later.\\n\\n          No results matched your search\\n\\nSearch for issues and pull requests\\n\\n#\\n\\nSearch for issues, pull requests, discussions, and projects\\n\\n#\\n\\nSearch for organizations, repositories, and users\\n\\n@\\n\\nSearch for projects\\n\\n!\\n\\nSearch for files\\n\\n/\\n\\nActivate command mode\\n\\n>\\n\\nSearch your issues, pull requests, and discussions\\n\\n# author:@me\\n\\nSearch your issues, pull requests, and discussions\\n\\n# author:@me\\n\\nFilter to pull requests\\n\\n# is:pr\\n\\nFilter to issues\\n\\n# is:issue\\n\\nFilter to discussions\\n\\n# is:discussion\\n\\nFilter to projects\\n\\n# is:project\\n\\nFilter to open issues, pull requests, and discussions\\n\\n# is:open\\n\\ndream\\n\\nPublic template\\n\\nEdit Pins\\n\\nPin to… \\n\\nProfile\\nPin this to your personal profile, visible to everyone\\n\\n    Cancel\\n     Apply\\n\\nWatch\\n\\n            Couldn\\'t load subscription status.\\xa0\\n\\nRetry\\n\\nFork\\n          67\\n Fork your own copy of deeppavlov/dream\\n\\n              Starred\\n 136\\n\\nLists\\n\\n              Star\\n 136\\n\\nLists\\n\\n                Notifications\\n\\n                      Participating and @mentions\\n\\n                      Only receive notifications from this repository when participating or @mentioned.\\n\\n                      All Activity\\n\\n                      Notified of all notifications on this repository.\\n\\n                      Ignore\\n\\n                      Never be notified.\\n\\nCustom\\n\\n                    Select events you want to be notified of in addition to participating and @mentions.\\n\\n                    Get push notifications on iOS or Android.\\n\\n                Custom\\n\\n                      Select events you want to be notified of in addition to participating and @mentions.\\n\\n                        Issues\\n\\n                        Pull requests\\n\\n                        Releases\\n\\n                        Discussions\\n\\n                        Security alerts\\n\\nApply\\n\\nCancel\\n\\nNotification settings\\n\\n Fork your own copy of deeppavlov/dream\\n\\n Unstar this repository\\n\\n Star this repository\\n\\n        DeepPavlov Dream is a free, open-source Multiskill AI Assistant Platform built using DeepPavlov Conversational AI Stack. It is built on top of DeepPavlov Agent running as container in Docker. It runs on x86_64 machines, and prefers having NVIDIA GPUs on the machine.\\n\\nwww.deeppavlov.ai/dream\\n\\nLicense\\n\\n     Apache-2.0 license\\n\\n136\\n    stars\\n\\n67\\n    forks\\n\\n11\\n    watching\\n\\nActivity\\n\\n    Public template repository\\n\\nOpen in github.dev\\nOpen in a new github.dev tab\\nOpen in codespace\\ndeeppavlov/dream\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\nmain\\n\\nSwitch branches/tags\\n\\nBranches\\nTags\\n\\nView all branches\\n\\nView all tags\\n\\nName already in use\\n\\n      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?\\n\\n    Cancel\\n\\n    Create\\n\\n117\\nbranches\\n\\n61\\ntags\\n\\nCode\\n\\nLocal\\n\\n Codespaces\\nNew\\n\\n  Clone\\n\\n              HTTPS\\n\\n              SSH\\n\\n              GitHub CLI\\n\\n        Use Git or checkout with SVN using the web URL.\\n\\n        You don\\'t have any public SSH keys in your GitHub account.\\n        You can add a new public key, or try cloning this repository via HTTPS.\\n\\n        Use a password-protected SSH key.\\n\\n      Work fast with our official CLI.\\n      Learn more about the CLI.\\n\\n    Open with GitHub Desktop\\n\\n    Download ZIP\\n\\nLaunching GitHub Desktop\\n\\n    If nothing happens, download GitHub Desktop and try again.\\n\\nLaunching GitHub Desktop\\n\\n    If nothing happens, download GitHub Desktop and try again.\\n\\nLaunching Xcode\\n\\n    If nothing happens, download Xcode and try again.\\n\\nLaunching Visual Studio Code\\nYour codespace will open once ready.\\nThere was a problem preparing your codespace, please try again.\\n\\n          Use this template\\n\\nCreate a new repository\\n\\n        Open in a codespace\\n\\nBranches\\n\\nTags\\n\\nLatest commit\\n\\ndilyararimovna\\n\\nMerge pull request #485 from deeppavlov/dev\\n\\n        …\\n\\n        a0ff368\\n\\nJun 15, 2023\\n\\nMerge pull request #485 from deeppavlov/dev\\n\\nRelease v1.6.0\\n\\na0ff368\\n\\nGit stats\\n\\n3,725\\n\\n                      commits\\n\\nFiles\\nPermalink\\n\\n    Failed to load latest commit information.\\n\\nType\\nName\\nLatest commit message\\nCommit time\\n\\n.github/workflows\\n\\nFix/black style (#409)\\n\\nApril 25, 2023 16:16\\n\\nannotators\\n\\ncombined mtl lightweight (#484)\\n\\nJune 13, 2023 12:15\\n\\nassistant_dists\\n\\nFeat/utilize llm resp selector (#483)\\n\\nJune 13, 2023 13:22\\n\\nbin\\n\\ndff_book_skill refactoring (#58)\\n\\nDecember 7, 2021 21:28\\n\\ncommon\\n\\nfeat: add chatgpt 16k (#491)\\n\\nJune 15, 2023 18:23\\n\\ncomponents\\n\\nfeat: add chatgpt 16k (#491)\\n\\nJune 15, 2023 18:23\\n\\ncore\\n\\ncommonb: fix style\\n\\nAugust 21, 2021 20:00\\n\\ndp\\n\\nDockerfiles refactoring (#375)\\n\\nMay 31, 2023 18:21\\n\\nmodels\\n\\ncommonb: fix style\\n\\nAugust 21, 2021 20:00\\n\\nresponse_selectors\\n\\nFeat/utilize llm resp selector (#483)\\n\\nJune 13, 2023 13:22\\n\\nservices\\n\\nfeat: add chatgpt 16k (#491)\\n\\nJune 15, 2023 18:23\\n\\nskill_selectors\\n\\ngoogle-api-skill utilizing LangChain\\'s google api wrapper (#377)\\n\\nJune 5, 2023 16:30\\n\\nskills\\n\\nfeat: add chatgpt 16k (#491)\\n\\nJune 15, 2023 18:23\\n\\nstate_formatters\\n\\nFeat/description based skill selector (#463)\\n\\nMay 24, 2023 18:22\\n\\ntests\\n\\nFeat/utilize llm resp selector (#483)\\n\\nJune 13, 2023 13:22\\n\\nutils\\n\\nupdate entity detection (#134)\\n\\nApril 29, 2022 23:00\\n\\n.dockerignore\\n\\nfix: rename program-y skills (#86)\\n\\nJanuary 27, 2022 14:08\\n\\n.env\\n\\nFix/update ports (#472)\\n\\nMay 29, 2023 09:57\\n\\n.env_ru\\n\\nFeat/ru llama distribution (#383)\\n\\nApril 19, 2023 17:07\\n\\n.flake8\\n\\nfeat/game_cooperative_skill (#677)\\n\\nApril 19, 2020 17:59\\n\\n.gitignore\\n\\nAdd compose definitions to component cards (#384)\\n\\nApril 30, 2023 15:37\\n\\n.travis.yml\\n\\nmove agent tests to Jenkins and on gpu\\n\\nOctober 4, 2019 14:38\\n\\nCONTRIBUTING.md\\n\\nfeat: upd deeppavlov links (#200)\\n\\nSeptember 19, 2022 13:46\\n\\nDREAM.png\\n\\nFeat/readme (#66)\\n\\nDecember 29, 2021 20:50\\n\\nJenkinsfile\\n\\nfeat: tests of new prompted demo dists (#480)\\n\\nJune 8, 2023 19:04\\n\\nLICENSE\\n\\nInitial commit\\n\\nMay 13, 2019 18:12\\n\\nLink_friendship_analysis.ipynb\\n\\nfeat: add notebook for link analysis (#1405)\\n\\nJune 15, 2021 00:00\\n\\nMODELS.md\\n\\nfeat: add chatgpt 16k (#491)\\n\\nJune 15, 2023 18:23\\n\\nREADME.md\\n\\ncombined mtl lightweight (#484)\\n\\nJune 13, 2023 12:15\\n\\nREADME_multilingual.md\\n\\nFeat/demo distributions full (#328)\\n\\nMarch 4, 2023 20:39\\n\\nREADME_ru.md\\n\\nfeat: new proxy address (#417)\\n\\nApril 26, 2023 14:12\\n\\nRussianDream.png\\n\\nfeat: first russian dream (#176)\\n\\nJune 30, 2022 14:36\\n\\nagent_settings.py\\n\\nfeat/agent/state_reset_for_first_turn (#801)\\n\\nApril 22, 2021 12:55\\n\\ncomponents.tsv\\n\\nfeat: add chatgpt 16k (#491)\\n\\nJune 15, 2023 18:23\\n\\ndb_conf.json\\n\\nMerge agent dev 23.01 (#345)\\n\\nJanuary 31, 2020 14:17\\n\\ndev_requirements.txt\\n\\nNew multitask 9in1 (Singlelabel version) (#213)\\n\\nJanuary 24, 2023 16:10\\n\\ndocker-compose.yml\\n\\nfix: move files to multimodal distr (#301)\\n\\nJanuary 26, 2023 16:07\\n\\ndockerfile_agent\\n\\nFeat/description based skill selector (#463)\\n\\nMay 24, 2023 18:22\\n\\nlog_config.yml\\n\\n- better deeppavlov images build - copy only necessary files (#370)\\n\\nFebruary 3, 2020 15:11\\n\\nmultilingualDream.png\\n\\nDream multilingual (#184)\\n\\nOctober 9, 2022 19:20\\n\\npublic_distributions.txt\\n\\nFeat/multiskill assistant (#434)\\n\\nApril 26, 2023 23:50\\n\\nunused_skills.yml\\n\\nFeat/service ports names (#339)\\n\\nMarch 13, 2023 15:19\\n\\n    View code\\n\\nDrop to upload your files\\n\\nDeepPavlov Dream\\nDistributions\\nDeepy Base\\nDeepy Advanced\\nDeepy FAQ\\nDeepy GoBot\\nDream\\nDream Mini\\nDream Russian\\nPrompted Dream Distributions\\nQuick Start\\nClone the repo\\nInstall docker and docker-compose\\nRun one of the Dream distributions\\nDeepy Base\\nDeepy Advanced\\nDeepy FAQ\\nDeepy GoBot\\nDream (via proxy)\\nDream (locally)\\nPrompted Dream\\nLet\\'s chat\\nCLI\\nHTTP API\\nTelegram Bot\\nConfiguration and proxy usage\\nComponents English Version\\nAnnotators\\nServices\\nSkills\\nPrompted Skills\\nPapers\\nAlexa Prize 3\\nAlexa Prize 4\\nLicense\\nReport creating\\n\\nREADME.md\\n\\nDeepPavlov Dream\\nDeepPavlov Dream is a platform for creating multi-skill chatbots.\\nTo get architecture documentation, please refer to DeepPavlov Agent readthedocs documentation.\\nDistributions\\nWe\\'ve already included six distributions: four of them are based on lightweight Deepy socialbot,\\none is a full-sized Dream chatbot (based on Alexa Prize Challenge version) in English and a Dream chatbot in Russian.\\nDeepy Base\\nBase version of Lunar assistant.\\nDeepy Base contains Spelling Preprocessing annotator,\\ntemplate-based Harvesters Maintenance Skill,\\nand AIML-based open-domain Program-y Skill based on Dialog Flow Framework.\\nDeepy Advanced\\nAdvanced version of Lunar assistant.\\nDeepy Advanced contains Spelling Preprocessing, Sentence Segmentation,\\nEntity Linking and Intent Catcher annotators, Harvesters Maintenance GoBot Skill for goal-oriented responses,\\nand AIML-based open-domain Program-y Skill based on Dialog Flow Framework.\\nDeepy FAQ\\nFAQ version of Lunar assistant.\\nDeepy FAQ contains Spelling Preprocessing annotator,\\ntemplate-based Frequently Asked Questions Skill,\\nand AIML-based open-domain Program-y Skill based on Dialog Flow Framework.\\nDeepy GoBot\\nGoal-oriented version of Lunar assistant.\\nDeepy GoBot Base contains Spelling Preprocessing annotator,\\nHarvesters Maintenance GoBot Skill for goal-oriented responses,\\nand AIML-based open-domain Program-y Skill based on Dialog Flow Framework.\\nDream\\nFull version of DeepPavlov Dream Socialbot.\\nThis is almost the same version of the DREAM socialbot as at\\nthe end of Alexa Prize Challenge 4.\\nSome API services are replaced with trainable models.\\nSome services (e.g., News Annotator, Game Skill, Weather Skill) require private keys for underlying APIs,\\nmost of them can be obtained for free.\\nIf you want to use these services in local deployments, add your keys to the environmental variables (e.g., ./.env, ./.env_ru).\\nThis version of Dream Socialbot consumes a lot of resources\\nbecause of its modular architecture and original goals (participation in Alexa Prize Challenge).\\nWe provide a demo of Dream Socialbot on our website.\\nDream Mini\\nMini version of DeepPavlov Dream Socialbot.\\nThis is a generative-based socialbot that uses English DialoGPT model to generate most of the responses. It also contains intent catcher and responder components to cover special user requests.\\nLink to the distribution.\\nDream Russian\\nRussian version of DeepPavlov Dream Socialbot. This is a generative-based socialbot that uses Russian DialoGPT by DeepPavlov to generate most of the responses. It also contains intent catcher and responder components to cover special user requests.\\nLink to the distribution.\\nPrompted Dream Distributions\\nMini version of DeepPavlov Dream Socialbot with the use of prompt-based generative models.\\nThis is a generative-based socialbot that uses large language models to generate most of the responses.\\nYou can upload your own prompts (json files) to common/prompts,\\nadd prompt names to PROMPTS_TO_CONSIDER (comma-separated),\\nand the provided information will be used in LLM-powered reply generation as a prompt.\\nLink to the distribution.\\nQuick Start\\nClone the repo\\ngit clone https://github.com/deeppavlov/dream.git\\n\\nInstall docker and docker-compose\\nIf you get a \"Permission denied\" error running docker-compose, make sure to configure your docker user correctly.\\nRun one of the Dream distributions\\nDeepy Base\\ndocker-compose -f docker-compose.yml -f assistant_dists/deepy_base/docker-compose.override.yml up --build\\n\\nDeepy Advanced\\ndocker-compose -f docker-compose.yml -f assistant_dists/deepy_adv/docker-compose.override.yml up --build\\n\\nDeepy FAQ\\ndocker-compose -f docker-compose.yml -f assistant_dists/deepy_faq/docker-compose.override.yml up --build\\n\\nDeepy GoBot\\ndocker-compose -f docker-compose.yml -f assistant_dists/deepy_gobot_base/docker-compose.override.yml up --build\\n\\nDream (via proxy)\\nThe easiest way to try out Dream is to deploy it via proxy.\\nAll the requests will be redirected to DeepPavlov API, so you don\\'t have to use any local resources.\\nSee proxy usage for details.\\ndocker-compose -f docker-compose.yml -f assistant_dists/dream/docker-compose.override.yml -f assistant_dists/dream/dev.yml -f assistant_dists/dream/proxy.yml up --build\\n\\nDream (locally)\\nPlease note, that DeepPavlov Dream components require a lot of resources.\\nRefer to the components section to see estimated requirements.\\ndocker-compose -f docker-compose.yml -f assistant_dists/dream/docker-compose.override.yml -f assistant_dists/dream/dev.yml up --build\\n\\nWe\\'ve also included a config with GPU allocations for multi-GPU environments:\\nAGENT_PORT=4242 docker-compose -f docker-compose.yml -f assistant_dists/dream/docker-compose.override.yml -f assistant_dists/dream/dev.yml -f assistant_dists/dream/test.yml up\\n\\nWhen you need to restart particular docker container without re-building (make sure mapping in assistant_dists/dream/dev.yml is correct):\\nAGENT_PORT=4242 docker-compose -f docker-compose.yml -f assistant_dists/dream/docker-compose.override.yml -f assistant_dists/dream/dev.yml restart container-name\\n\\nPrompted Dream\\ndocker-compose -f docker-compose.yml -f assistant_dists/dream_persona_prompted/docker-compose.override.yml -f assistant_dists/dream_persona_prompted/dev.yml -f assistant_dists/dream_persona_prompted/proxy.yml up --build\\n\\nWe\\'ve also included a config with GPU allocations for multi-GPU environments.\\nLet\\'s chat\\nDeepPavlov Agent provides several options for interaction: a command line interface, an HTTP API, and a Telegram bot\\nCLI\\nIn a separate terminal tab run:\\ndocker-compose exec agent python -m deeppavlov_agent.run agent.channel=cmd agent.pipeline_config=assistant_dists/dream/pipeline_conf.json\\n\\nEnter your username and have a chat with Dream!\\nHTTP API\\nOnce you\\'ve started the bot, DeepPavlov\\'s Agent API will run on http://localhost:4242.\\nYou can learn about the API from the DeepPavlov Agent Docs.\\nA basic chat interface will be available at http://localhost:4242/chat.\\nTelegram Bot\\nCurrently, Telegram bot is deployed instead of HTTP API.\\nEdit agent command definition inside docker-compose.override.yml config:\\nagent:\\n  command: sh -c \\'bin/wait && python -m deeppavlov_agent.run agent.channel=telegram agent.telegram_token=<TELEGRAM_BOT_TOKEN> agent.pipeline_config=assistant_dists/dream/pipeline_conf.json\\'\\n\\nNOTE: treat your Telegram token as a secret and do not commit it to public repositories!\\nConfiguration and proxy usage\\nDream uses several docker-compose configuration files:\\n./docker-compose.yml is the core config which includes containers for DeepPavlov Agent and mongo database;\\n./assistant_dists/*/docker-compose.override.yml lists all components for the distribution;\\n./assistant_dists/dream/dev.yml includes volume bindings for easier Dream debugging;\\n./assistant_dists/dream/proxy.yml is a list of proxied containers.\\nIf your deployment resources are limited, you can replace containers with their proxied copies hosted by DeepPavlov.\\nTo do this, override those container definitions inside proxy.yml, e.g.:\\nconvers-evaluator-annotator:\\n  command: [\"nginx\", \"-g\", \"daemon off;\"]\\n  build:\\n    context: dp/proxy/\\n    dockerfile: Dockerfile\\n  environment:\\n    - PROXY_PASS=proxy.deeppavlov.ai:8004\\n    - SERVICE_PORT=8004\\n\\nand include this config in your deployment command:\\ndocker-compose -f docker-compose.yml -f assistant_dists/dream/docker-compose.override.yml -f assistant_dists/dream/dev.yml -f assistant_dists/dream/proxy.yml up --build\\n\\nBy default, proxy.yml contains all available proxy definitions.\\nComponents English Version\\nDream Architecture is presented in the following image:\\n\\nName\\nRequirements\\nDescription\\n\\nRule Based Selector\\n\\nAlgorithm that selects list of skills to generate candidate responses to the current context based on topics, entities, emotions, toxicity, dialogue acts and dialogue history\\n\\nResponse Selector\\n50 MB RAM\\nAlgorithm that selects a final responses among the given list of candidate responses\\n\\nAnnotators\\n\\nName\\nRequirements\\nDescription\\n\\nASR\\n40 MB RAM\\ncalculates overall ASR confidence for a given utterance and grades it as either very low, low, medium, or high (for Amazon markup)\\n\\nBadlisted Words\\n150 MB RAM\\ndetects words and phrases from the badlist\\n\\nCombined Classification\\n1.5 GB RAM, 3.5 GB GPU\\nBERT-based model including topic classification, dialog acts classification, sentiment, toxicity, emotion, factoid classification\\n\\nCombined Classification lightweight\\n1.6 GB RAM\\nThe same model as Combined Classification, but takes 42% less time thanks to the lighter backbone\\n\\nCOMeT Atomic\\n2 GB RAM, 1.1 GB GPU\\nCommonsense prediction models COMeT Atomic\\n\\nCOMeT ConceptNet\\n2 GB RAM, 1.1 GB GPU\\nCommonsense prediction models COMeT  ConceptNet\\n\\nConvers Evaluator Annotator\\n1 GB RAM, 4.5 GB GPU\\nis trained on the Alexa Prize data from the previous competitions and predicts whether the candidate response is interesting, comprehensible, on-topic, engaging, or erroneous\\n\\nEmotion Classification\\n2.5 GB RAM\\nemotion classification annotator\\n\\nEntity Detection\\n1.5 GB RAM, 3.2 GB GPU\\nextracts entities and their types from utterances\\n\\nEntity Linking\\n2.5 GB RAM, 1.3 GB GPU\\nfinds Wikidata entity ids for the entities detected with Entity Detection\\n\\nEntity Storer\\n220 MB RAM\\na rule-based component, which stores entities from the user\\'s and socialbot\\'s utterances if opinion expression is detected with patterns or MIDAS Classifier and saves them along with the detected attitude to dialogue state\\n\\nFact Random\\n50 MB RAM\\nreturns random facts for the given entity (for entities from user utterance)\\n\\nFact Retrieval\\n7.4 GB RAM, 1.2 GB GPU\\nextracts facts from Wikipedia and wikiHow\\n\\nIntent Catcher\\n1.7 GB RAM, 2.4 GB GPU\\nclassifies user utterances into a number of predefined intents which are trained on a set of phrases and regexps\\n\\nKBQA\\n2 GB RAM, 1.4 GB GPU\\nanswers user\\'s factoid questions based on Wikidata KB\\n\\nMIDAS Classification\\n1.1 GB RAM, 4.5 GB GPU\\nBERT-based model trained on a semantic classes subset of MIDAS dataset\\n\\nMIDAS Predictor\\n30 MB RAM\\nBERT-based model trained on a semantic classes subset of MIDAS dataset\\n\\nNER\\n2.2 GB RAM, 5 GB GPU\\nextracts person names, names of locations, organizations from uncased text\\n\\nNews API Annotator\\n80 MB RAM\\nextracts the latest news about entities or topics using the GNews API. DeepPavlov Dream deployments utilize our own API key.\\n\\nPersonality Catcher\\n30 MB RAM\\nthe skill is to change the system\\'s personality description via chatting interface, it works as a system command, the response is system-like message\\n\\nPrompt Selector\\n50 MB RAM\\nAnnotator utilizing Sentence Ranker to rank prompts and selecting N_SENTENCES_TO_RETURN most relevant prompts (based on questions provided in prompts)\\n\\nProperty Extraction\\n6.3 GiB RAM\\nextracts user attributes from utterances\\n\\nRake Keywords\\n40 MB RAM\\nextracts keywords from utterances with the help of RAKE algorithm\\n\\nRelative Persona Extractor\\n50 MB RAM\\nAnnotator utilizing Sentence Ranker to rank persona sentences and selecting N_SENTENCES_TO_RETURN the most relevant sentences\\n\\nSentrewrite\\n200 MB RAM\\nrewrites user\\'s utterances by replacing pronouns with specific names that provide more useful information to downstream components\\n\\nSentseg\\n1 GB RAM\\nallows us to handle long and complex user\\'s utterances by splitting them into sentences and recovering punctuation\\n\\nSpacy Nounphrases\\n180 MB RAM\\nextracts nounphrases using Spacy and filters out generic ones\\n\\nSpeech Function Classifier\\n1.1 GB RAM, 4.5 GB GPU\\na hierarchical algorithm based on several linear models and a rule-based approach for the prediction of speech functions described by Eggins and Slade\\n\\nSpeech Function Predictor\\n1.1 GB RAM, 4.5 GB GPU\\nyields probabilities of speech functions that can follow a speech function predicted by Speech Function Classifier\\n\\nSpelling Preprocessing\\n50 MB RAM\\npattern-based component to rewrite different colloquial expressions to a more formal style of conversation\\n\\nTopic Recommendation\\n40 MB RAM\\noffers a topic for further conversation using the information about the discussed topics and user\\'s preferences. Current version is based on Reddit personalities (see Dream Report for Alexa Prize 4).\\n\\nToxic Classification\\n3.5 GB RAM, 3 GB GPU\\nToxic classification model from Transformers specified as PRETRAINED_MODEL_NAME_OR_PATH\\n\\nUser Persona Extractor\\n40 MB RAM\\ndetermines which age category the user belongs to based on some key words\\n\\nWiki Parser\\n100 MB RAM\\nextracts Wikidata triplets for the entities detected with Entity Linking\\n\\nWiki Facts\\n1.7 GB RAM\\nmodel that extracts related facts from Wikipedia and WikiHow pages\\n\\nServices\\n\\nName\\nRequirements\\nDescription\\n\\nDialoGPT\\n1.2 GB RAM, 2.1 GB GPU\\ngenerative service based on Transformers generative model, the model is set in docker compose argument PRETRAINED_MODEL_NAME_OR_PATH (for example, microsoft/DialoGPT-small with 0.2-0.5 sec on GPU)\\n\\nDialoGPT Persona-based\\n1.2 GB RAM, 2.1 GB GPU\\ngenerative service based on Transformers generative model, the model was pre-trained on the PersonaChat dataset to generate a response conditioned on a several sentences of the socialbot\\'s persona\\n\\nImage Captioning\\n4 GB RAM, 5.4 GB GPU\\ncreates text representation of a received image\\n\\nInfilling\\n1  GB RAM, 1.2 GB GPU\\n(turned off but the code is available) generative service based on Infilling model, for the given utterance returns utterance where _ from original text is replaced with generated tokens\\n\\nKnowledge Grounding\\n2 GB RAM, 2.1 GB GPU\\ngenerative service based on BlenderBot architecture providing a response to the context taking into account an additional text paragraph\\n\\nMasked LM\\n1.1 GB RAM, 1 GB GPU\\n(turned off but the code is available)\\n\\nSeq2seq Persona-based\\n1.5 GB RAM, 1.5 GB GPU\\ngenerative service based on Transformers seq2seq model, the model was pre-trained on the PersonaChat dataset to generate a response conditioned on a several sentences of the socialbot\\'s persona\\n\\nSentence Ranker\\n1.2 GB RAM, 2.1 GB GPU\\nranking model given as PRETRAINED_MODEL_NAME_OR_PATH which for a pair os sentences returns a float score of correspondence\\n\\nStoryGPT\\n2.6 GB RAM, 2.15 GB GPU\\ngenerative service based on fine-tuned GPT-2, for the given set of keywords returns a short story using the keywords\\n\\nGPT-3.5\\n100 MB RAM\\ngenerative service based on OpenAI API service, the model is set in docker compose argument PRETRAINED_MODEL_NAME_OR_PATH (in particular, in this service, text-davinci-003 is used.\\n\\nChatGPT\\n100 MB RAM\\ngenerative service based on OpenAI API service, the model is set in docker compose argument PRETRAINED_MODEL_NAME_OR_PATH (in particular, in this service, gpt-3.5-turbo is used.\\n\\nPrompt StoryGPT\\n3 GB RAM, 4 GB GPU\\ngenerative service based on fine-tuned GPT-2, for the given topic represented by one noun returns short story on a given topic\\n\\nGPT-J 6B\\n1.5 GB RAM, 24.2 GB GPU\\ngenerative service based on Transformers generative model, the model is set in docker compose argument PRETRAINED_MODEL_NAME_OR_PATH (in particular, in this service, GPT-J model is used.\\n\\nBLOOMZ 7B\\n2.5 GB RAM, 29 GB GPU\\ngenerative service based on Transformers generative model, the model is set in docker compose argument PRETRAINED_MODEL_NAME_OR_PATH (in particular, in this service, BLOOMZ-7b1 model is used.\\n\\nGPT-JT 6B\\n2.5 GB RAM, 25.1 GB GPU\\ngenerative service based on Transformers generative model, the model is set in docker compose argument PRETRAINED_MODEL_NAME_OR_PATH (in particular, in this service, GPT-JT model is used.\\n\\nSkills\\n\\nName\\nRequirements\\nDescription\\n\\nAlexa Handler\\n30 MB RAM\\nhandler for several specific Alexa commands\\n\\nChristmas Skill\\n30 MB RAM\\nsupports FAQ, facts, and scripts for Christmas\\n\\nComet Dialog skill\\n300 MB RAM\\nuses COMeT ConceptNet model to express an opinion, to ask a question or give a comment about user\\'s actions mentioned in the dialogue\\n\\nConvert Reddit\\n1.2 GB RAM\\nuses a ConveRT encoder to build efficient representations for sentences\\n\\nDummy Skill\\na part of agent container\\na fallback skill with multiple non-toxic candidate responses\\n\\nDummy Skill Dialog\\n600 MB RAM\\nreturns the next turn from the Topical Chat dataset if the response of the user to the Dummy Skill is similar to the corresponding response in the source data\\n\\nEliza\\n30 MB RAM\\nChatbot (https://github.com/wadetb/eliza)\\n\\nEmotion Skill\\n40 MB RAM\\nreturns template responses to emotions detected by Emotion Classification from Combined Classification annotator\\n\\nFactoid QA\\n170 MB RAM\\nanswers factoid questions\\n\\nGame Cooperative Skill\\n100 MB RAM\\nprovides user with a conversation about computer games: the charts of the best games for the past year, past month, and last week\\n\\nHarvesters Maintenance Skill\\n30 MB RAM\\nHarvesters maintenance skill\\n\\nHarvesters Maintenance Gobot Skill\\n30 MB RAM\\nHarvesters maintenance Goal-oriented skill\\n\\nKnowledge Grounding Skill\\n100 MB RAM\\ngenerates a response based on the dialogue history and provided knowledge related to the current conversation topic\\n\\nMeta Script Skill\\n150 MB RAM\\nprovides a multi-turn dialogue around human activities. The skill uses COMeT Atomic model to generate commonsensical descriptions and questions on several aspects\\n\\nMisheard ASR\\n40 MB RAM\\nuses the ASR Processor annotations to give feedback to the user when ASR confidence is too low\\n\\nNews API Skill\\n60 MB RAM\\npresents the top-rated latest news about entities or topics using the GNews API\\n\\nOscar Skill\\n30 MB RAM\\nsupports FAQ, facts, and scripts for Oscar\\n\\nPersonal Info Skill\\n40 MB RAM\\nqueries and stores user\\'s name, birthplace, and location\\n\\nDFF Program Y Skill\\n800 MB RAM\\n[New DFF version] Chatbot Program Y (https://github.com/keiffster/program-y) adapted for Dream socialbot\\n\\nDFF Program Y Dangerous Skill\\n100 MB RAM\\n[New DFF version] Chatbot Program Y (https://github.com/keiffster/program-y) adapted for Dream socialbot, containing responses to dangerous situations in a dialog\\n\\nDFF Program Y Wide Skill\\n110 MB RAM\\n[New DFF version] Chatbot Program Y (https://github.com/keiffster/program-y) adapted for Dream socialbot, which includes only very general templates (with lower confidence)\\n\\nSmall Talk Skill\\n35 MB RAM\\nasks questions using the hand-written scripts for 25 topics, including but not limited to love, sports, work, pets, etc.\\n\\nSuperBowl Skill\\n30 MB RAM\\nsupports FAQ, facts, and scripts for SuperBowl\\n\\nText QA\\n1.8 GB RAM, 2.8 GB GPU\\nThe service finds the answer of a factoid question in text.\\n\\nValentine\\'s Day Skill\\n30 MB RAM\\nsupports FAQ, facts, and scripts for Valentine\\'s Day\\n\\nWikidata Dial Skill\\n100 MB RAM\\ngenerates an utterance using Wikidata triplets. Not turned on, needs improvement\\n\\nDFF Animals Skill\\n200 MB RAM\\nis created using DFF and has three branches of conversation about animals: user\\'s pets, pets of the socialbot, and wild animals\\n\\nDFF Art Skill\\n100 MB RAM\\nDFF-based skill to discuss art\\n\\nDFF Book Skill\\n400 MB RAM\\n[New DFF version] detects book titles and authors mentioned in the user\\'s utterance with the help of Wiki parser and Entity linking and recommends books by leveraging information from the GoodReads database\\n\\nDFF Bot Persona Skill\\n150 MB RAM\\naims to discuss user favorites and 20 most popular things with short stories expressing the socialbot\\'s opinion towards them\\n\\nDFF Coronavirus Skill\\n110 MB RAM\\n[New DFF version] retrieves data about the number of coronavirus cases and deaths in different locations sourced from the John Hopkins University Center for System Science and Engineering\\n\\nDFF Food Skill\\n150 MB RAM\\nconstructed with DFF to encourage food-related conversation\\n\\nDFF Friendship Skill\\n100 MB RAM\\n[New DFF version] DFF-based skill to greet the user in the beginning of the dialog, and forward the user to some scripted skill\\n\\nDFF Funfact Skill\\n100 MB RAM\\n[New DFF version] Tells user fun facts\\n\\nDFF Gaming Skill\\n80 MB RAM\\nprovides a video games discussion. Gaming Skill is for more general talk about video games\\n\\nDFF Gossip Skill\\n95 MB RAM\\nDFF-based skill to discuss other people with news about them\\n\\nDFF Image Skill\\n100 MB RAM\\n[New DFF version] Scripted skill that based on the sent image captioning (from annotations) responses with specified responses in case of food, animals or people detected, and default responses otherwise\\n\\nDFF Template Skill\\n50 MB RAM\\n[New DFF version] DFF-based skill that provides an example of DFF usage\\n\\nDFF Template Prompted Skill\\n50 MB RAM\\n[New DFF version] DFF-based skill that provides answers generated by language model based on specified prompts and the dialog context. The model to be used is specified in GENERATIVE_SERVICE_URL. For example, you may use Transformer LM GPTJ service.\\n\\nDFF Grounding Skill\\n90 MB RAM\\n[New DFF version] DFF-based skill to answer what is the topic of the conversation, to generate acknowledgement, to generate universal responses on some dialog acts by MIDAS\\n\\nDFF Intent Responder\\n100 MB RAM\\n[New DFF version]  provides template-based replies for some of the intents detected by Intent Catcher annotator\\n\\nDFF Movie Skill\\n1.1 GB RAM\\nis implemented using DFF and takes care of the conversations related to movies\\n\\nDFF Music Skill\\n70 MB RAM\\nDFF-based skill to discuss music\\n\\nDFF Science Skill\\n90 MB RAM\\nDFF-based skill to discuss science\\n\\nDFF Short Story Skill\\n90 MB RAM\\n[New DFF version] tells user short stories from 3 categories: (1) bedtime stories, such as fables and moral stories, (2) horror stories, and (3) funny ones\\n\\nDFF Sport Skill\\n70 MB RAM\\nDFF-based skill to discuss sports\\n\\nDFF Travel Skill\\n70 MB RAM\\nDFF-based skill to discuss travel\\n\\nDFF Weather Skill\\n1.4 GB RAM\\n[New DFF version] uses the OpenWeatherMap service to get the forecast for the user\\'s location\\n\\nDFF Wiki Skill\\n150 MB RAM\\nused for making scenarios with the extraction of entities, slot filling, facts insertion, and acknowledgements\\n\\nPrompted Skills\\n\\nName\\nRequirements\\nDescription\\n\\nAI FAQ Skill\\n150 MB RAM\\n[New DFF version] Everything you wanted to know about modern AI but was afraid to ask! This FAQ Assistant chats with you while explaining the simplest topics from today\\'s technology world.\\n\\nFashion Stylist Skill\\n150 MB RAM\\n[New DFF version] Stay protected in every season with da Costa Industries Clothes Assistant! Experience the ultimate comfort and protection, no matter the weather. Stay warm in winter a...\\n\\nDream Persona Skill\\n150 MB RAM\\n[New DFF version] Prompt-based skill that utilizes given generative service to generate responses based on the given prompt\\n\\nMarketing Skill\\n150 MB RAM\\n[New DFF version] Connect with your audience like never before with Marketing AI Assistant! Reach new heights of success by tapping into the power of empathy. Say goodbye..\\n\\nFairytale Skill\\n150 MB RAM\\n[New DFF version] This assistant will tell you or your children a short but engaging fairytale. Choose the characters and the topic and leave the rest to AI imagination.\\n\\nNutrition Skill\\n150 MB RAM\\n[New DFF version] Discover the secret to healthy eating with our AI assistant! Find nutritious food options for you and your loved ones with ease. Say goodbye to mealtime stress and hello to delici...\\n\\nLife Coaching Skill\\n150 MB RAM\\n[New DFF version] Unlock your full potential with Rhodes & Co\\'s patented AI assistant! Reach peak performance at work and at home. Get into top form effortlessly and inspire others with.\\n\\nPapers\\nAlexa Prize 3\\nKuratov Y. et al. DREAM technical report for the Alexa Prize 2019 //Alexa Prize Proceedings. – 2020.\\nAlexa Prize 4\\nBaymurzina D. et al. DREAM Technical Report for the Alexa Prize 4 //Alexa Prize Proceedings. – 2021.\\nLicense\\nDeepPavlov Dream is licensed under Apache 2.0.\\nProgram-y (see dream/skills/dff_program_y_skill, dream/skills/dff_program_y_wide_skill, dream/skills/dff_program_y_dangerous_skill)\\nis licensed under Apache 2.0.\\nEliza (see dream/skills/eliza) is licensed under MIT License.\\nReport creating\\nFor making certification xlsx - file with bot responses, you can use xlsx_responder.py script by executing\\ndocker-compose -f docker-compose.yml -f dev.yml exec -T -u $(id -u) agent python3 \\\\\\n        utils/xlsx_responder.py --url http://0.0.0.0:4242 \\\\\\n        --input \\'tests/dream/test_questions.xlsx\\' \\\\\\n        --output \\'tests/dream/output/test_questions_output.xlsx\\'\\\\\\n      --cache tests/dream/output/test_questions_output_$(date --iso-8601=seconds).json\\nMake sure all services are deployed. --input - xlsx file with certification questions, --output - xlsx file with bot responses, --cache - json, that contains a detailed markup and is used for a cache.\\n\\nAbout\\n\\n      DeepPavlov Dream is a free, open-source Multiskill AI Assistant Platform built using DeepPavlov Conversational AI Stack. It is built on top of DeepPavlov Agent running as container in Docker. It runs on x86_64 machines, and prefers having NVIDIA GPUs on the machine.\\n\\nwww.deeppavlov.ai/Dream\\n\\nTopics\\n\\n  docker\\n\\n  chatbot\\n\\n  hactoberfest\\n\\nResources\\n\\n        Readme\\n\\nLicense\\n\\n     Apache-2.0 license\\n\\nActivity\\n\\nStars\\n\\n136\\n    stars\\n\\nWatchers\\n\\n11\\n    watching\\n\\nForks\\n\\n67\\n    forks\\n\\n        Report repository\\n\\n    Releases\\n      61\\n\\nRelease v1.6.0\\n\\n          Latest\\n\\nJun 15, 2023\\n\\n        + 60 releases\\n\\n    Packages\\n      0\\n\\n        No packages published \\nPublish your first package\\n\\n    Contributors\\n      49\\n\\n      + 38 contributors\\n\\nLanguages\\n\\nPython\\n90.2%\\n\\nJupyter Notebook\\n6.0%\\n\\nDockerfile\\n2.2%\\n\\nShell\\n1.2%\\n\\nPerl\\n0.3%\\n\\nHTML\\n0.1%\\n\\nFooter\\n\\n        © 2023 GitHub, Inc.\\n\\nFooter navigation\\n\\nTerms\\nPrivacy\\nSecurity\\nStatus\\nDocs\\nContact GitHub\\nPricing\\nAPI\\nTraining\\nBlog\\nAbout\\n\\n    You can’t perform that action at this time.\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "with open('dream_repo.html', 'r') as f:\n",
    "    html = f.read()\n",
    "soup = BeautifulSoup(html)\n",
    "html_text_draft = soup.get_text()\n",
    "html_text = re.sub(r'(\\n\\s*)+\\n+', '\\n\\n', html_text_draft)\n",
    "html_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
